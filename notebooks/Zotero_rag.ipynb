{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zotero RAG \n",
    "## Chat with Your Zotero Library\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "Before running this notebook, you need to create a `.env` file in the root directory of the project with your Zotero credentials:\n",
    "\n",
    "```\n",
    "ZOTERO_API_KEY=your_zotero_api_key_here\n",
    "ZOTERO_USER_ID=your_zotero_user_id_here\n",
    "LIBRARY_TYPE=user\n",
    "GROQ_API_KEY=your_groq_api_key_here\n",
    "```\n",
    "\n",
    "### How to get your Zotero credentials:\n",
    "1. **API Key**: Go to https://www.zotero.org/settings/keys/new and create a new private key\n",
    "2. **User ID**: Found in your Zotero account settings or in the URL when you visit your library (e.g., https://www.zotero.org/users/YOUR_USER_ID/)\n",
    "3. **Library Type**: Use `user` for personal library or `group` for group library\n",
    "\n",
    "### How to get your Groq API key (if you don't go for ollama local option):\n",
    "1. Visit https://console.groq.com/\n",
    "2. Sign up or log in\n",
    "3. Navigate to API Keys section and create a new key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyzotero import zotero\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()  # by default, looks for `.env` in the working dir\n",
    "\n",
    "# ZOTERO_API_KEY = os.getenv(\"ZOTERO_API_KEY\", \"\").strip()\n",
    "# ZOTERO_USER_ID = os.getenv(\"ZOTERO_USER_ID\", \"\").strip()\n",
    "# LIBRARY_TYPE = os.getenv(\"LIBRARY_TYPE\", \"\").strip()\n",
    "\n",
    "# if not ZOTERO_API_KEY or not ZOTERO_USER_ID or not LIBRARY_TYPE:\n",
    "#     raise ValueError(\"Zotero API key, user ID, and library type must be set in the environment variables.\")\n",
    "\n",
    "# print(f\"Library Type: '{LIBRARY_TYPE}'\")  # Debug print to verify the value\n",
    "# print(f\"User ID: '{ZOTERO_USER_ID}'\")  # Debug print to verify the value\n",
    "\n",
    "# # Initialize Zotero client\n",
    "# zot = zotero.Zotero(ZOTERO_USER_ID, LIBRARY_TYPE, ZOTERO_API_KEY)\n",
    "\n",
    "# # Get items with attachments (PDFs)\n",
    "# items = zot.items(top=True, itemType='attachment')\n",
    "\n",
    "# download_folder = \"../documents/zotero_pdfs\"\n",
    "# os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# for item in items:\n",
    "#     if 'application/pdf' in item['data'].get('contentType', ''):\n",
    "#         title = item['data']['title']\n",
    "#         key = item['key']\n",
    "#         try:\n",
    "#             file = zot.file(key)\n",
    "#             path = os.path.join(download_folder, title + \".pdf\")\n",
    "#             with open(path, \"wb\") as f:\n",
    "#                 f.write(file)\n",
    "#             print(f\"Downloaded: {title}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to download {title}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Download Zotero PDFs\n",
    "\n",
    "The cell above downloads PDFs from your Zotero library. Uncomment and run it once.\n",
    "\n",
    "**Note:** Some PDFs may fail to download if they are:\n",
    "- Linked files stored locally on your computer (not synced to Zotero cloud)\n",
    "- PDFs without proper access permissions\n",
    "- Attachments that were deleted or moved\n",
    "\n",
    "If any PDFs failed to download, you can manually add them to the `../documents/zotero_pdfs/` folder to include them in your RAG system.\n",
    "The section and RAG are independent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Using Ollama (local, no token limits) - before, I used Groq but it was too limited for a normal size Zotero library\n",
    "# First install Ollama: https://ollama.com/download\n",
    "# Then run in terminal: ollama pull llama3.2\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Alternative: Use Groq (cloud-based, fast but has token limits)\n",
    "# Uncomment the lines below and comment out the Ollama lines above to use Groq\n",
    "# \n",
    "# from langchain_groq import ChatGroq\n",
    "# llm = ChatGroq(\n",
    "#     model=\"llama3-8b-8192\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 231 pages from 10 PDF files\n"
     ]
    }
   ],
   "source": [
    "# Load all PDFs from Zotero folder\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import glob\n",
    "\n",
    "# Get all PDF files from the zotero_pdfs folder\n",
    "pdf_files = glob.glob(\"../documents/zotero_pdfs/*.pdf\")\n",
    "\n",
    "if not pdf_files:\n",
    "    raise ValueError(\"No PDF files found in ../documents/zotero_pdfs/. Please download PDFs first.\")\n",
    "\n",
    "# Load all documents from all PDFs\n",
    "all_documents = []\n",
    "for pdf_path in pdf_files:\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    all_documents.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(all_documents)} pages from {len(pdf_files)} PDF files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1378 chunks from the documents\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=800, chunk_overlap=80):\n",
    "    \"\"\"\n",
    "    this function splits documents into chunks of given size and overlap\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents=documents)\n",
    "    return chunks\n",
    "\n",
    "zotero_chunks = split_documents(all_documents)\n",
    "print(f\"Created {len(zotero_chunks)} chunks from the documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def create_embedding_vector_db(chunks, db_name):\n",
    "    \"\"\"\n",
    "    this function uses the open-source embedding model HuggingFaceEmbeddings \n",
    "    to create embeddings and store those in a vector database called FAISS, \n",
    "    which allows for efficient similarity search\n",
    "    \"\"\"\n",
    "    # instantiate embedding model\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "    )\n",
    "    # create the vector store \n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding\n",
    "    )\n",
    "    # save vector database locally\n",
    "    vectorstore.save_local(f\"../vector_databases/vector_db_{db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and save vector database\n",
    "create_embedding_vector_db(chunks=zotero_chunks, db_name=\"zotero\")\n",
    "print(\"Vector database created and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve from Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_vector_db(vector_db_path):\n",
    "    \"\"\"\n",
    "    this function splits out a retriever object from a local vector database\n",
    "    \"\"\"\n",
    "    # instantiate embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "    )\n",
    "    zotero_vectorstore = FAISS.load_local(\n",
    "        folder_path=vector_db_path,\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    # Retrieve more documents since Ollama has no strict token limits\n",
    "    retriever = zotero_vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "zotero_retriever = retrieve_from_vector_db(\"../vector_databases/vector_db_zotero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_chains(retriever):\n",
    "    \"\"\"\n",
    "    this function connects stuff_documents_chain with retrieval_chain\n",
    "    \"\"\"\n",
    "    stuff_documents_chain = create_stuff_documents_chain(\n",
    "        llm=llm,\n",
    "        prompt=hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "    )\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=stuff_documents_chain\n",
    "    )\n",
    "    return retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "zotero_retrieval_chain = connect_chains(zotero_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(\n",
    "    inquiry,\n",
    "    retrieval_chain=zotero_retrieval_chain\n",
    "):\n",
    "    result = retrieval_chain.invoke({\"input\": inquiry})\n",
    "    print(result['answer'].strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with Your Zotero Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can summarize the main topics covered in the papers:\n",
      "\n",
      "1. **Language Models**: The papers discuss various aspects of language models, including their performance, limitations, and potential societal impacts. For example, \"LoRA: Low-Rank Adaptation of Large Language Models\" explores a method for adapting large language models to specific tasks, while \"The Automation Charade\" critiques the increasing reliance on AI in automation.\n",
      "\n",
      "2. **AI Research and Ethics**: The papers touch on the importance of ethics in AI research, including the need for transparency, accountability, and human-centered design. For instance, \"Ethically aligned design: A vision for prioritizing human well-being with artificial intelligence and autonomous systems\" proposes a framework for designing AI systems that prioritize human well-being.\n",
      "\n",
      "3. **AI Governance and Standards**: The papers discuss the need for governance and standards in AI development, including the importance of ensuring accountability, transparency, and fairness. For example, \"Democratize AI? How the Proposed National AI Research Resource Falls Short\" critiques a proposed national AI research resource for its limitations.\n",
      "\n",
      "4. **Generative AI**: The papers cover various aspects of generative AI, including its potential applications, limitations, and societal implications. For instance, \"The Gradient of Generative AI Release: Methods and Considerations\" explores the challenges and opportunities associated with releasing generative AI models.\n",
      "\n",
      "5. **AI and Society**: The papers discuss the broader social implications of AI development, including issues related to job displacement, bias, and transparency. For example, \"What Is AI? Part 2, with Lucy Suchman\" provides an introduction to AI and its potential impact on society.\n",
      "\n",
      "6. **Technical Papers**: The papers also cover technical topics in AI research, such as the stability of AI funds, the generation of code using GitHub Copilot, and the importance of transparency in AI development.\n",
      "\n",
      "Overall, the papers cover a range of topics related to AI development, ethics, governance, and societal implications.\n"
     ]
    }
   ],
   "source": [
    "# Example: Ask a question about your Zotero library\n",
    "print_output(\"Give me a summary of the main topics covered in the papers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are the key findings and conclusions from the papers:\n",
      "\n",
      "1. \"The Hardware Lottery\" by Sara Hooker (2020):\n",
      "The paper discusses the challenges of developing large language models (LLMs) that can be deployed on a variety of hardware platforms. The author argues that the current approach to LLM development is like playing a \"hardware lottery,\" where the model's performance depends on the specific hardware configuration.\n",
      "\n",
      "Key finding: The author highlights the need for more research on how to develop LLMs that can adapt to different hardware environments.\n",
      "\n",
      "2. \"LoRA: Low-Rank Adaptation of Large Language Models\" by Edward J. Hu et al. (2021):\n",
      "The paper proposes a new method called LoRA, which adapts large language models to specific hardware platforms using low-rank matrix factorization. The authors demonstrate that LoRA can improve the performance of LLMs on various hardware configurations.\n",
      "\n",
      "Key finding: The authors show that LoRA can reduce the computational requirements and memory usage of LLMs, making them more suitable for deployment on resource-constrained devices.\n",
      "\n",
      "3. \"Democratize AI? How the Proposed National AI Research Resource Falls Short\" by Amba Kak et al. (2021):\n",
      "The paper critiques the proposed national AI research resource, arguing that it may not effectively democratize access to AI technology. The authors highlight concerns about data privacy, bias, and the potential for unequal distribution of benefits.\n",
      "\n",
      "Key finding: The authors conclude that the proposed national AI research resource falls short in addressing the needs of marginalized communities and ensuring equitable access to AI technology.\n",
      "\n",
      "4. \"Mozilla's AI Investing in Trustworthy AI\" by K. Nuortimo et al. (no specific date mentioned):\n",
      "The paper discusses Mozilla's efforts to invest in trustworthy AI, including the development of a new AI research foundation. The authors highlight the importance of ensuring that AI systems are transparent, explainable, and fair.\n",
      "\n",
      "Key finding: The authors emphasize the need for more research on trustworthy AI and its applications in various domains.\n",
      "\n",
      "5. \"Heartbleed Bug\" by Synopsys (no specific date mentioned):\n",
      "The paper discusses the Heartbleed bug, a security vulnerability discovered in the OpenSSL library. The authors provide an overview of the bug's impact on online security and discuss potential mitigation strategies.\n",
      "\n",
      "Key finding: The authors highlight the importance of regular security audits and testing to prevent similar vulnerabilities from occurring in the future.\n",
      "\n",
      "6. \"Meta Is Spinning off Its PyTorch Framework into Its Own AI Research Foundation\" by Andrew Tarantola (no specific date mentioned):\n",
      "The paper reports on Meta's decision to spin off its PyTorch framework into a new AI research foundation. The author discusses the potential implications of this move for the AI research community.\n",
      "\n",
      "Key finding: The author notes that the spin-off may lead to increased competition and innovation in the field of AI research.\n",
      "\n",
      "7. \"The Automation Charade\" by Astra Taylor (no specific date mentioned):\n",
      "The paper critiques the growing use of automation technology, arguing that it can exacerbate existing social and economic inequalities. The author discusses potential solutions for mitigating the negative impacts of automation.\n",
      "\n",
      "Key finding: The author concludes that automation raises important questions about the future of work and the need for more equitable policies to address its consequences.\n",
      "\n",
      "8. \"Google Appeals Huge Android Antitrust Fine to EU's Top Court\" by Chan (no specific date mentioned):\n",
      "The paper reports on Google's appeal of a large antitrust fine imposed by the European Union. The author discusses the implications of this decision for the tech industry and regulatory policy.\n",
      "\n",
      "Key finding: The author notes that the appeal highlights ongoing debates about the role of competition law in regulating the tech industry.\n",
      "\n",
      "9. \"PyTorch vs TensorFlow: Who Has More Pre-Trained Deep Learning Models?\" by Foster (no specific date mentioned):\n",
      "The paper compares the number of pre-trained deep learning models available for PyTorch and TensorFlow. The author discusses the implications of this comparison for researchers and developers.\n",
      "\n",
      "Key finding: The author concludes that PyTorch has a larger collection of pre-trained models, which may make it more attractive to some users.\n",
      "\n",
      "10. \"Meta's LLaMa 2 License Is Not Open Source\" by Stefano Maffulli (no specific date mentioned):\n",
      "The paper reports on Meta's decision not to open-source its LLaMa 2 model. The author discusses the implications of this move for the AI research community and the potential consequences for innovation.\n",
      "\n",
      "Key finding: The author notes that Meta's decision may limit access to the benefits of LLaMa 2, potentially hindering progress in the field of natural language processing.\n",
      "\n",
      "11. \"On Behalf of the Young Kenyans Whose Lives Have Been Ruined Because They Did the Dirty Work Training the #ChatGPT Algorithm\" by Mercy Sumbi (no specific date mentioned):\n",
      "The paper reports on a petition filed by young Kenyans who claim to have been exploited in the development of ChatGPT. The author discusses the implications of this case for labor rights and AI ethics.\n",
      "\n",
      "Key finding: The author highlights the need for greater transparency and accountability in the development of AI systems, particularly when it comes to issues of labor exploitation.\n",
      "\n",
      "12. \"Google Just Open Sourced TensorFlow, Its Artificial Intelligence Engine | WIRED\" by Cade Metz (no specific date mentioned):\n",
      "The paper reports on Google's decision to open-source its TensorFlow framework. The author discusses the implications of this move for the AI research community and the potential benefits for innovation.\n",
      "\n",
      "Key finding: The author notes that the open-sourcing of TensorFlow may lead to increased collaboration and progress in the field of AI research.\n",
      "\n",
      "These papers cover a range of topics related to AI, including language models, trustworthy AI, automation, and labor rights. They highlight key findings and conclusions on issues such as data privacy, bias, and the need for more equitable policies to address the consequences of automation.\n"
     ]
    }
   ],
   "source": [
    "# Ask another question\n",
    "print_output(\"What are the key findings or conclusions from the papers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask any custom question about your Zotero library\n",
    "# print_output(\"Your question here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
